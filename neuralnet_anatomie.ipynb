{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RalfH1388/genai-lecture/blob/main/neuralnet_anatomie.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NARTDzmf4O0n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AE4ovxKn35-6"
      },
      "outputs": [],
      "source": [
        "# Neuronale Netze\n",
        "# ---------------\n",
        "\n",
        "# Im Folgenden betrachten wir ein einfaches Beispiel eines Neuronalen Netzes -\n",
        "# keine kompexere Variante wie z.B. rekurrente Netze (mit Gewichten innerhalb\n",
        "# einer Schicht oder gar zu vorherigen Schichten), und keine Deep Learning-Netze\n",
        "# (mit mehr als einer Zwischenschicht). Des Weiteren verzichten wir auf die\n",
        "# Verwendung eines sog. Bias und einer sog. Lernarte, die normalerweise\n",
        "# ebenfalls zu Neuronalen Netzen gehören.\n",
        "# Die Vorgehensweise eines Neuronalen Netzes ist - je komplexer es wird -\n",
        "# immer noch weniger vom Nutzer nachvollziehbar, bzw. ist schlicht zur Laufzeit\n",
        "# eine \"black box\". Wir fokussieren uns daher auf ein ganz einfaches Beispiel,\n",
        "# um eine Idee davon zu bekommen, was in einem Neuronalen Netz passiert.\n",
        "# Wir werden keinen vorgefertigten NN-Algorithmus zur Validierung unseres\n",
        "# eigenen Codes verwenden - ein so \"einfaches\" Beispiel ist damit kaum zu\n",
        "# replizieren, und diese Beispiel ist ohnehin bereits komplex genug."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Wir wollen nun ein neuronales Netz mit folgender Struktur erstellen:\n",
        "# - Inputschicht: 2 Neuronen\n",
        "# - Zwischenschicht: 3 Neuronen\n",
        "# - Outputschicht: 1 Neuron\n",
        "# Es soll jedes Neuron mit jedem Neuron mittels eines Gewichtes verbunden sein.\n",
        "# Die Gewichte werden ursprünglich mit random-Zahlen initialisiert.\n",
        "# Wir simulieren ein Training, bestehend aus nur EINER Abfolge von Feedforward\n",
        "# und Backpropagation, und wir trainieren einen Datensatz mit nur EINER Zeile.\n",
        "# Diese Zeile besteht aus den Werten 1 und 0 als Input, und 0 als Label.\n",
        "# Die Aktivierungsfunktion der Neuronen soll die Sigmoid-Funktion sein."
      ],
      "metadata": {
        "id": "ABPzRw7T4uAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hierfür brauchen wir grundsätzlich wieder den Klassiker numpy,\n",
        "# sowie das Paket random, um zufällige initiale Gewicht belegen zu können.\n",
        "import numpy as np\n",
        "import random"
      ],
      "metadata": {
        "id": "xfHXjM0Oh0jr"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unsere Aktivierungsfunktion (also die Funktion, die das Eingangssignal eines\n",
        "# Neurons zum tatsächlichen Wert des Neurons umwandelt) ist die Sigmoidfunktion:\n",
        "# f(z)=1/(1+e^(-z)). z ist hierbei das Eingangssignal.\n",
        "\n",
        "def sigmoid(X):\n",
        "    return 1/(1 + np.exp(-X))\n",
        "\n",
        "# Für die Bestimmung der backpropagation-Gewichte brauchen wir u.a. die\n",
        "# partielle Ableitung der Sigmoid-Funktion (Herleitung ist hier nicht nötig).\n",
        "def ableitung_sigmoid(X):\n",
        "    return sigmoid(X) * (1 - sigmoid(X))"
      ],
      "metadata": {
        "id": "DhXmohFc4mAG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bauen wir zunächst das Neuronale Netz selbst, als eine Klasse NeuronalesNetz,\n",
        "# aus der wir später ein Objekt dieser Klasse instanziieren können.\n",
        "class NeuronalesNetz:\n",
        "\n",
        "  # Konstruktor plus Attribute der Klasse\n",
        "  def __init__(self, X, Y):\n",
        "    # Input-Daten in der Inputschicht\n",
        "    self.input = X\n",
        "    # Output-Daten\n",
        "    self.output = Y\n",
        "    # Gewichte von der Inputschicht zur Zwischenschicht\n",
        "    # (das sind 3 Inputschicht-Neuronen mal 4 Zwischenschichten-Neuronen =\n",
        "    # 12 Gewichte)\n",
        "    self.gewichte_input_zwischen = np.random.random_sample((self.input.shape[1],4))\n",
        "    # Gewicht von der Zwischenschicht zur Outputschicht\n",
        "    # (das sind 4 Zwischenschichten-Neuronen mal 1 Outputschicht-Neuron =\n",
        "    # 4 Gewichte)\n",
        "    self.gewichte_zwischen_output = np.random.random_sample((4,1))\n",
        "    # Vorhersage des Neuronalen Netzes in der Outputschicht,\n",
        "    # zunächst mit Nullern befüllt\n",
        "    self.output_vorhersage = np.zeros(self.output.shape)\n",
        "\n",
        "  # Nun müssen wir den feedforward-Prozess als Funktionen programmieren, sprich\n",
        "  # die Berechnung der Signale z:\n",
        "\n",
        "  def feedforward(self):\n",
        "    # Die Werte der 4 Zwischenschicht-Neuronen werden berechnet mit\n",
        "    # sigmoid(z)=1/(1+e^-z), mit z=W*x, wobei W die Gewichte von der Input-\n",
        "    # zur Zwischenschicht sind, und x die Inputdaten\n",
        "    self.werte_zwischen = sigmoid(np.dot(self.input, self.gewichte_input_zwischen))\n",
        "    # Der Wert des Output-Neurons wird ebenfalls berechnet mit\n",
        "    # sigmoid(z)=1/(1+e^-z), mit z=W*x, wobei W diesmal Gewichte von der\n",
        "    # Zwischen- zur Outputschicht sind, und x die zuvor berechneten Werte der\n",
        "    # 4 Zwischenschicht-Neuronen\n",
        "    self.output_vorhersage = sigmoid(np.dot(self.werte_zwischen, self.gewichte_zwischen_output))\n",
        "\n",
        "  # Nun müssen wir den backpropagation-Prozess als Funktion programmieren,\n",
        "  # sprich die Berechnung der Veränderung der Gewichte, sodass der Verlust\n",
        "  # minimal wird\n",
        "  def backpropagation(self):\n",
        "    # Die Anpassung der 4 Gewichte von der Zwischen- zur Outputschicht haben\n",
        "    # wir explizit von Hand berechnet: Die Ableitung der Verlustfunktion nach\n",
        "    # den Gewichten ist das Produkt aus drei einzelnen partiellen Ableitungen:\n",
        "    # - 1. die Ableitung der Verlustfunktion nach dem vorhergesagten Output\n",
        "    #      (2*(self.output - self.output_vorhersage)),\n",
        "    # - 2. die Ableitung des vorhergesagten Outputs nach dem im Output-Neuron\n",
        "    #      ankommenden Signal - also nichts anderes als die Ableitung der\n",
        "    #      Sigmoid-Funktion (ableitung_sigmoid(self.output)), und\n",
        "    # - 3. die Ableitung des im Output-Neuron ankommenden Signals nach den\n",
        "    #      Gewichten von der Zwischenschicht zur Outputschicht\n",
        "    #      (self.werte_zwischen.T):\n",
        "    # Die Anpassung der 4 Gewichter von der Zwischen- zur Outputschicht\n",
        "    # geschieht folgendermaßen:\n",
        "    ableitung_gewichte_zwischen_output = np.dot(self.werte_zwischen.T, (-2*(self.output - self.output_vorhersage) * ableitung_sigmoid(self.output_vorhersage)))\n",
        "    # Die Anpassung der 12 Gewichte von der Input- zur Zwischenschicht wird auf\n",
        "    # die selbe Weise berechnet, haben wir aber aus Zeitgründen nicht\n",
        "    # explizit gemacht:\n",
        "    ableitung_gewichte_input_zwischen = np.dot(self.input.T, (np.dot(-2*(self.output - self.output_vorhersage) * ableitung_sigmoid(self.output_vorhersage), self.gewichte_zwischen_output.T) * ableitung_sigmoid(self.werte_zwischen)))\n",
        "    # Nun müssen die Gewichte überschrieben werden: Die neuen Gewichte sind die\n",
        "    # alten Gewichte plus der Verändung der Gewichte, wie sie genau durch die\n",
        "    # Ableitung repräsentiert wird.\n",
        "    self.gewichte_zwischen_output = self.gewichte_zwischen_output - ableitung_gewichte_zwischen_output\n",
        "    self.gewichte_input_zwischen = self.gewichte_input_zwischen - ableitung_gewichte_input_zwischen\n"
      ],
      "metadata": {
        "id": "fBV8XG5piq-x"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Um die Ergebnisse wieder replizierbar zu machen, verwenden wir sog.\n",
        "# Pseudo-Zufallszahlen, d.h. es werden bei Euch die selben Zufallszahlen gesetzt\n",
        "# wie bei mir, sofern Ihr in der Klammer die selbe Zahl habt wie ich -\n",
        "# welche Zahl das ist, spielt keine Rolle.\n",
        "np.random.seed(1)\n",
        "\n",
        "# Nun erzeugen wir Daten. Der Einfachheit halber nur einen Datensatz mit drei\n",
        "# Input-Werten (für jedes Neuron in der Inputschicht einen), und einen\n",
        "# Output-Wert.\n",
        "if __name__ == \"__main__\":\n",
        "  X = np.array([[0,0,1]])\n",
        "  Y = np.array([[0]])\n",
        "\n",
        "# Nun erzeugen wir ein Objekt der Klasse Neuonales Netzwerk:\n",
        "  NN = NeuronalesNetz(X,Y)\n",
        "\n",
        "# Nun machen wir erstmal lediglich eine Iteration der for-Schleife, um unsere\n",
        "# von-Hand-Berechnungen zu verifizieren. Ihr könnt später gerne range()\n",
        "# auf 1.000 oder höher stellen, um zu sehen, dass die Vorhersage des Y-Werts 0\n",
        "# immer genauer wird!\n",
        "\n",
        "# Für die von-Hand-Berechnung können wir uns hier (also nach der Erzeugung\n",
        "# des Objekts, aber noch vor feedforward und backpropagation) die initialen,\n",
        "# pseudo-zufällig gewählten Gewichte holen. Davor zwingen wir Python noch zur\n",
        "# normalen Formatierung von Kommazahlen:\n",
        "  np.set_printoptions(suppress=True)\n",
        "  print(NN.gewichte_input_zwischen)\n",
        "  print(NN.gewichte_zwischen_output)\n",
        "  for i in range(10):\n",
        "    NN.feedforward()\n",
        "    NN.backpropagation()\n",
        "\n",
        "# Zur Sicherstellung können die von Hand gerechneten Werte der\n",
        "# Zwischenschicht-Neuronen hiermit überprüft werden:\n",
        "  print(NN.werte_zwischen)\n",
        "\n",
        "# Ebenso kann überprüft werden, ob die neuen Gewichte von der Zwischen- zur\n",
        "# Outputschicht richtig berechnet wurden:\n",
        "  print(NN.gewichte_zwischen_output)\n",
        "\n",
        "# Nun können wir uns danach die Vorhersage ausgeben. Diese ist ca. 0,7575\n",
        "# (also noch weit weg von der tatsächlichen 0, was nicht überraschend ist,\n",
        "# weil der erste Durchlauf noch mit zufälligen, nicht-verbesserten Gewichten\n",
        "# durchgeführt wurde). Hier kann ebenfalls überprüft werden, ob die von-Hand-\n",
        "# Berechnung der Vorhersage des Neuronalen Netzes richtig ist:\n",
        "  print(NN.output_vorhersage)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaXEqDLsidEW",
        "outputId": "ae541ebb-189f-4301-8a31-dd4bf9b17527"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.417022   0.72032449 0.00011437 0.30233257]\n",
            " [0.14675589 0.09233859 0.18626021 0.34556073]\n",
            " [0.39676747 0.53881673 0.41919451 0.6852195 ]]\n",
            "[[0.20445225]\n",
            " [0.87811744]\n",
            " [0.02738759]\n",
            " [0.67046751]]\n",
            "[[0.62133829 0.59280913 0.64341152 0.64952056]]\n",
            "[[-0.89204667]\n",
            " [-0.22740906]\n",
            " [-1.09313778]\n",
            " [-0.51531432]]\n",
            "[[0.16862495]]\n"
          ]
        }
      ]
    }
  ]
}