{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gL6fUbyJ63yE"
      },
      "source": [
        "Wir speichern den API Key ab und laden ein Modell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PCT3FXdH63yF"
      },
      "outputs": [],
      "source": [
        "#mit Secret Manager in Google Colab\n",
        "from google.colab import userdata\n",
        "OPENAI_API_KEY = userdata.get('secret_name')\n",
        "\n",
        "#Youtube-Video\n",
        "#\"https://www.youtube.com/watch?v=cdiD-9MMpb0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "oAx9JZ6063yG"
      },
      "outputs": [],
      "source": [
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgF4Ji8l63yG"
      },
      "source": [
        "Wir stellen jetzt dem Modell eine simple Frage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Wsb4hw863yH",
        "outputId": "eeb6d307-d5a7-40f3-f5aa-4d82538d2251"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Die Hauptstadt von Deutschland ist Berlin.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 14, 'total_tokens': 22, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_01aeff40ea', 'finish_reason': 'stop', 'logprobs': None}, id='run-f46912c8-4e46-49ee-870b-77fe0b7dc655-0', usage_metadata={'input_tokens': 14, 'output_tokens': 8, 'total_tokens': 22, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.invoke(\"Was ist die Hauptstadt von Deutschland?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbVhb8EL63yI"
      },
      "source": [
        "Der Rückgabewert ist ein AIMessage-Objekt. Mit einem Output Parser können wir die Antwort daraus extrahieren."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "E5jLhZR163yI",
        "outputId": "acbf055b-5504-45f9-e148-8debffcaf88f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Die Hauptstadt von Berlin ist Berlin selbst. Berlin ist eine Stadt und ein Bundesland in Deutschland und hat somit den Status einer Hauptstadt.'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "chain = model | parser\n",
        "chain.invoke(\"Was ist die Hauptstadt von Deutschland?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-akFLsGh63yJ"
      },
      "source": [
        "Wir wollen jetzt dem Modell einen Kontext und eine Frage übergeben."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "jgsQCYeE63yJ",
        "outputId": "cd48df4b-b555-41d9-f870-46674c8cd2e5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Human: \\nAnswer the question based on the context below. If you can\\'t \\nanswer the question, reply \"I don\\'t know\".\\n\\nContext: Ralfs Bruder heißt Axl\\n\\nQuestion: Wie heißt Ralfs Bruder?\\n'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "template = \"\"\"\n",
        "Answer the question based on the context below. If you can't\n",
        "answer the question, reply \"I don't know\".\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "prompt.format(context=\"Ralfs Bruder heißt Axl\", question=\"Wie heißt Ralfs Bruder?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rtaU33463yJ"
      },
      "source": [
        "Wir nutzen jetzt langchain, um dem Prompt mit dem Modell und dem Parser zu verketten, d.h. der Output des Prompts ist Input des Modells, und der Output des Modells ist Input des Parsers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "id": "A4BmBu9e63yK",
        "outputId": "c9e0d46e-bbc4-4601-9e0f-c9d23222aa63"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Ralfs Bruder heißt Axl.'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain = prompt | model | parser\n",
        "chain.invoke({\n",
        "    \"context\": \"Ralfs Bruder heißt Axl.\",\n",
        "    \"question\": \"Wie heißt Ralfs Bruder?\"\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FigpX0qD63yK"
      },
      "source": [
        "Wir können sogar Ketten kombinieren, um komplexere Workflows zu bauen. Beispielsweise können wir eine zweite Kette definieren, die die Antwort der ersten Kette in eine andere Sprache übersetzt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5g6Ibmx263yL"
      },
      "outputs": [],
      "source": [
        "translation_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Übersetze {answer} in {language}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxCP9hxK63yL"
      },
      "source": [
        "Wir können jetzt eine neue Übersetzungs-Kette bauen die das Ergebnis der ersten Kette kombiniert mit dem Übersetzungs-Prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "id": "3qG6IYff63yL",
        "outputId": "ad5ca7cd-366b-4e55-8627-43b09c2d725c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Ralf has a total of three siblings.'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from operator import itemgetter\n",
        "\n",
        "translation_chain = (\n",
        "    {\"answer\": chain, \"language\": itemgetter(\"language\")} | translation_prompt | model | parser\n",
        ")\n",
        "\n",
        "translation_chain.invoke(\n",
        "    {\n",
        "        \"context\": \"Ralfs Bruder heißt Axl. Er hat zwei weitere Geschwister.\",\n",
        "        \"question\": \"Wie viele Geschwister hat Ralf?\",\n",
        "        \"language\": \"Englisch\",\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5x2-ql7563yM"
      },
      "source": [
        "Wir lesen die Transkription des o.g. youtube-Videos ein, und zeigen die ersten 100 Zeichen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "aso9MXRo63yM",
        "outputId": "bcc5e50b-b2ed-4465-96f8-162eb2f11845"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"I think it's possible that physics has exploits and we should be trying to find them. arranging some\""
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with open(\"interview.txt\") as file:\n",
        "  interview = file.read()\n",
        "\n",
        "interview[:100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsHaVBWB63yN"
      },
      "source": [
        "WIr können jetzt dem Modell das gesamte Interview als Kontext übergeben. Allerdings hängt es vom Modell und dessen erlaubter context size ab, ob es auch funktioniert."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "gE3Z9xsq63yN",
        "outputId": "8f0e8c31-45e3-4196-be10-df36fdf3e454"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Yes, reading papers is a good idea, especially in the field of AI and machine learning. It allows you to stay updated on the latest research, understand new methodologies, and gain insights into the current state of the field. Papers often provide detailed explanations and experimental results that can enhance your knowledge and inform your work. However, it's important to recognize that the practical application and implementation of ideas in real-world scenarios are also crucial, and it's beneficial to complement reading papers with hands-on experimentation and coding.\""
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke({\n",
        "    \"context\": transcription,\n",
        "    \"question\": \"Is reading papers a good idea?\"\n",
        "    })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjwA751n63yN"
      },
      "source": [
        "Selbst wenn das Modell den kompletten Kontext aufnehmen kann, macht es Sinn, diesen zu splitten, und nur den relevanten Kontext mitzuliefern. Wir bilden also Chunks der Dokumente und wollen dem Modell anschließend nur die relevanten Chunks mitgeben.\n",
        "\n",
        "Dazu laden wir zunächst aus Vereinfachungsgründen das gesamte Interview in den Speicher:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "TeFk1D6u63yN"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import TextLoader\n",
        "\n",
        "loader = TextLoader(\"interview.txt\")\n",
        "text_documents = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94kocCye63yO"
      },
      "source": [
        "Es gibt viele verschiedene Möglichkeiten, ein Dokuments in Chunks zu splitten. Für dieses Beispiel nutzen wir einen simplen Splitter, der die Dokumente in Chunks mit einer fixierten Größe aufteilt.\n",
        "\n",
        "In diesem Beispiel splitten wir in Chunks mit einer Größe von 100 Zeichen und einem Overlap von 20 Zeichen, um den Kontext zwischen Sätzen nicht zu verlieren."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zA1iY0Y63yO",
        "outputId": "400e25ad-e6af-4c44-8270-d8f6f9544081"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'interview.txt'}, page_content=\"I think it's possible that physics has exploits and we should be trying to find them. arranging some\"),\n",
              " Document(metadata={'source': 'interview.txt'}, page_content='arranging some kind of a crazy quantum mechanical system that somehow gives you buffer overflow,'),\n",
              " Document(metadata={'source': 'interview.txt'}, page_content='buffer overflow, somehow gives you a rounding error in the floating point. Synthetic intelligences'),\n",
              " Document(metadata={'source': 'interview.txt'}, page_content=\"intelligences are kind of like the next stage of development. And I don't know where it leads to.\"),\n",
              " Document(metadata={'source': 'interview.txt'}, page_content='where it leads to. Like at some point, I suspect the universe is some kind of a puzzle. These')]"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
        "text_splitter.split_documents(text_documents)[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ePt0aap63yO"
      },
      "source": [
        "Für unseren speziellen Anwendungsfall verwenden wir 1000:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "NT9efG8363yO"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
        "documents = text_splitter.split_documents(text_documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiiclwHl63yP"
      },
      "source": [
        "Gegeben eine bestimmte Frage, müssen wir die relevanten Chunks aus dem Interview finden, die wir dann an das Modell senden. Hier kommt das Konzept der Embeddings ins Spiel.\n",
        "\n",
        "Ein Embedding ist eine mathematisch Repräsentation der semantischen Bedeutung eines Wortes, Sätzes oder Dokumentes. Es ist quasi eine Projektion eines Konzeptes in einem höherdimensionalen Raum. Embeddings haben eine simple Eigenschaft: Die Projektion von verwandten Konzepten sind nahe beieinander, während Konzepte mit unterschiedlichen Bedeutungen weiter voneinander entfernt liegen. Siehe auch https://dashboard.cohere.com/playground/embed, um Embeddings in zwei Dimensionen zu visualisieren.\n",
        "\n",
        "Um die relevantesten Chungs bereitzustellen, können wir das Embedding der Frage und der Chungs des Interviews nutzen, um die Ähnlichkeit zwischen ihnen zu berechnen. Anschließend können wir die Chunks mit der höchsten Ähnlichkeit zur Frage auswählen, um sie dem Modell zu geben.\n",
        "\n",
        "Lass uns voran ein Embedding für eine beliebige Frage erzeugen:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXfCAWjk63yP",
        "outputId": "4d883c2b-21bf-4d9c-efd1-9d03a521314f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding Länge: 1536\n",
            "[0.01599571667611599, -0.019886909052729607, 0.01371423527598381, -0.013169214129447937, -0.027580568566918373, 0.014462053775787354, -0.013625510968267918, -0.0007193002384155989, 0.0008008948643691838, -0.01744065433740616]\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
        "embedded_query = embeddings.embed_query(\"Wer ist Ralfs Bruder?\")\n",
        "\n",
        "print(f\"Embedding Länge: {len(embedded_query)}\")\n",
        "print(embedded_query[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCEJdVHk63yP"
      },
      "source": [
        "1536 beudetet in diesem Fall, dass das verwendete Embedding-Modell Text in einen 1536-dimensionalen Raum repräsentiert.\n",
        "\n",
        "Um zu illustrieren, wie Embeddings funktionieren, generieren wir zunächst Embeddings für zwei verschiedene Sätze."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "Ox5yHspZ63yP"
      },
      "outputs": [],
      "source": [
        "sentence1 = embeddings.embed_query(\"Ralfs Bruder ist Axl\")\n",
        "sentence2 = embeddings.embed_query(\"Michaelas Bruder ist Barbara\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVEaW4_363yP"
      },
      "source": [
        "Wir können jetzt die Ähnlichkeit zwischen der Query (Wer ist Ralfs Bruder?) und diesen beiden Sätzen berechnen. Je näher die Embeddings beieinander sind, desto ähnlicher sind sich die Query und der entsprechende Satz.\n",
        "\n",
        "Wir nutzen die Cosine Similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Zz1HDFW63yQ",
        "outputId": "c2d22126-7214-4613-f913-0da965df2084"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.9093964737341363, 0.8186437442297532)"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "query_sentence1_similarity = cosine_similarity([embedded_query], [sentence1])[0][0]\n",
        "query_sentence2_similarity = cosine_similarity([embedded_query], [sentence2])[0][0]\n",
        "\n",
        "query_sentence1_similarity, query_sentence2_similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_KPttA-63yQ"
      },
      "source": [
        "Wir sehen, dass die Ähnlichkeit des Satzes \"Ralfs Bruder ist Axl\" zum Satz \"Wer ist Ralfs Bruder?\" sehr hoch ist mit 0,9 (der Score bewegt sich zwischen -1 und 1), was auch Sinn ergibt. Die Ähnlichkeit zu \"Michaelas Schwester ist Barbara\" zum Satz \"Wer ist Ralfs Bruder?\" ist ebenfalls hoch, weil alle 3 Sätze etwas mit Geschwisterverhältnissen zu tun haben. Allerdings hat der sentence1 zusätzlich noch direkt etwas mit den beteiligten Personen der query zu tun, weshalb dessen Wert noch etwas höher ist.\n",
        "\n",
        "Wir brauchen jetzt einen effizienten Weg um die Chunks, deren Embeddings und deren Ähnlichkeits-Scores zu speichern. Dafür bedienen wir uns einer sog. Vektordatenbank. Eine Vektordatenbank ist eine Datenbank, die auf schnelle Ähnlichkeitssuchen spezialisiert ist.\n",
        "\n",
        "Um zu verstehen wie eine Vektordatenbank funktioniert, erstellen wir eine in-memory and fügen ihr ein paar Embeddings hinzu:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "aliAw9QS63yQ"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
        "\n",
        "vectorstore1 = DocArrayInMemorySearch.from_texts(\n",
        "    [\n",
        "        \"Ralfs Bruder ist Axl\",\n",
        "        \"Michaelas Schwester ist Barbara\",\n",
        "        \"Simon mag weiße Autos\",\n",
        "        \"Janniks Vater ist Anlagenbauer\",\n",
        "        \"Tassilo fährt einen Seat\",\n",
        "        \"Sara hat eine Schwester\",\n",
        "    ],\n",
        "    embedding=embeddings,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkGCO7WB63yR"
      },
      "source": [
        "Wir können jetzt die Vektordatenbank befragen um die ähnlichsten Embeddings zu finden, gegeben der Query, die wir eingeben."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUo4-09Q63yR",
        "outputId": "ef059b61-e516-4f6c-d475-66fa6289766e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(Document(metadata={}, page_content='Ralfs Bruder ist Axl'),\n",
              "  0.9093964827944877),\n",
              " (Document(metadata={}, page_content='Michaelas Schwester ist Barbara'),\n",
              "  0.8172974263814057),\n",
              " (Document(metadata={}, page_content='Sara hat eine Schwester'),\n",
              "  0.8146215355810805)]"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vectorstore1.similarity_search_with_score(query=\"Wer ist Ralfs Bruder?\", k=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3IQikQa63yR"
      },
      "source": [
        "Wir sehen, dass die 3 Sätze, die am meisten mit Geschwisterverhältnissen zu tun haben, zurückgeliefert werden.\n",
        "\n",
        "Wir können die Vektordatenbank benutzen, um die relevantesten Chunks des Interviews (gegeben einer bestimmten Frage) zu finden, um sie dem Modell zu übergeben. Wir können übrigens auch die Vektordatenbank mit einer Kette verbinden.\n",
        "\n",
        "Wir müssen einen Retriever konfigurieren. Der Retriever wird eine Ähnlichkeitssuche in der Vektordatenbank durchführen und die relevantesten Dokumente an den nächsten Schritt in der Kette weitergeben. Den Retriever können wir direkt von der Vektordatenbank bekommen, die wir zuvor erstellt haben."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7_24LmD63yR",
        "outputId": "4d4fc460-906f-44f1-a41a-366ee7c5b4ca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={}, page_content='Ralfs Bruder ist Axl'),\n",
              " Document(metadata={}, page_content='Michaelas Schwester ist Barbara'),\n",
              " Document(metadata={}, page_content='Sara hat eine Schwester'),\n",
              " Document(metadata={}, page_content='Janniks Vater ist Anlagenbauer')]"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retriever1 = vectorstore1.as_retriever()\n",
        "retriever1.invoke(\"Wer ist Ralfs Bruder?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZC0bTz163yS"
      },
      "source": [
        "Unser Prompt erwartet zwei Parameter, \"context\" und \"question\". Wir können den Retriever benutzen um die Chunks zu finden, die wir als den Kontext benutzen werden, um die Frage zu beantworten.\n",
        "\n",
        "Wir können hierzu eine map mit zwei Inputs erstellen, indem wir RunnableParallel und RunnablePassthrough nutzen. Dies erlaubt uns den Kontext und die Frage an den Prompt zu übergeben als eine map mit den keys \"context\" und \"question\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DF7lTY9H63yS",
        "outputId": "5df3749e-17df-4f4a-c0c0-267a36ec9af6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'context': [Document(metadata={}, page_content='Tassilo fährt einen Seat'),\n",
              "  Document(metadata={}, page_content='Simon mag weiße Autos'),\n",
              "  Document(metadata={}, page_content='Michaelas Schwester ist Barbara'),\n",
              "  Document(metadata={}, page_content='Ralfs Bruder ist Axl')],\n",
              " 'question': 'Welches Auto fährt Tassilo?'}"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
        "\n",
        "setup = RunnableParallel(context=retriever1, question=RunnablePassthrough())\n",
        "setup.invoke(\"Welches Auto fährt Tassilo?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U637tX3K63yS"
      },
      "source": [
        "Jetzt können wir die map der Kette hinzufügen und laufen lassen.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "id": "rv3AVFhB63yS",
        "outputId": "1ea41907-5577-485f-e9fe-6edc852bab6e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Tassilo fährt einen Seat.'"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain = setup | prompt | model | parser\n",
        "chain.invoke(\"Welches Auto fährt Tassilo?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7gLxwHu63yT"
      },
      "source": [
        "Wir können die Kette auch mit einer anderen Query aufrufen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "id": "9dWub8Ts63yT",
        "outputId": "3f7e731a-1a3b-4a5b-9524-8ae93c547de8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Janniks Vater ist Anlagenbauer.'"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke(\"Was ist Janniks Vater von Beruf?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTrIfhO_63yT"
      },
      "source": [
        "Oben haben wir die Vektordatenbank mit beliebigen Strings befüllt. Jetzt erzeugen wir eine neue Vektordatenbank für die Chunks des Interviews."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "cF9I3Qdi63yT"
      },
      "outputs": [],
      "source": [
        "vectorstore2 = DocArrayInMemorySearch.from_documents(documents, embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eO0PHYzf63yT"
      },
      "source": [
        "Wir können jetzt eine neue Kette erzeugen, indem wir die neue Vektordatenbank benutzen. Dieses Mal verwenden wir eine andere, aber äquivalente Syntax um den RunnableParallel-Teil der Kette zu spezifizieren."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "ZZ5kkzMI63yU",
        "outputId": "a7cfb713-3352-4de1-8fcc-4c8059742f8d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Synthetic intelligence refers to artificial intelligences that are seen as the next stage of development in technology. They are capable of learning, processing information, and potentially uncovering complex puzzles about the universe. These intelligences can generate content, engage in conversations about emotions, and exhibit reasoning by manipulating information and making predictions based on inputs.'"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain = (\n",
        "    {\"context\": vectorstore2.as_retriever(), \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | parser\n",
        ")\n",
        "chain.invoke(\"What is synthetic intelligence?\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
