{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RalfH1388/genai-lecture/blob/main/genai_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieval Augmented Generation (RAG)\n",
        "# ---\n",
        "# In diesem Beispiel erstellen wir ein RAG-System.\n",
        "# Warum ist RAG sinnvoll? Weil ein LLM nicht per se inhaltlich korrekten Inhalt\n",
        "# produziert, sondern wie ein \"stochastischer Papagei\" entlang seiner Trainings-\n",
        "# daten Tokens basierend auf vorherigen Tokens produziert.\n",
        "# Wenn wir uns sicher sein wollen, dass die Inhalte korrekt sind, brauchen wir\n",
        "# also andere Methoden. Und manchmal ist es ohnehin so, dass wir sehr spezielle\n",
        "# Informationen verarbeiten müssen (z.B. geheime firmeninterne Dokumente), von\n",
        "# denen das LLM sowieso nicht direkt etwas Bescheid weiß.\n",
        "# Die Grundidee eines RAG-Systems ist, dass ein Sprachmodell bei der\n",
        "# Beantwortung von Fragen externe Wissensquellen durchsucht und relevante\n",
        "# Informationen in seine Antwort integriert."
      ],
      "metadata": {
        "id": "UiQ1Su33M_Lf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "4c9Oe7l32gf0"
      },
      "outputs": [],
      "source": [
        "# Wir wollen hier mit den LLMs von OpenAI arbeiten. Da wir ein Backend-System\n",
        "# entwickeln, hilft uns die UI-Version von ChatGPT nicht weiter, sondern wir\n",
        "# brauchen Zugriff zur Developer-API mittels API Key. Jeder von Euch hat von mir\n",
        "# entweder einen API Key von meinem persönlichen Account bekommen, oder nutzt\n",
        "# einen aus seinem eigenen Account. Dieser Key muss - falls Ihr in Google\n",
        "# Colab bleibt - als Secret hier hinterlegt werden, ansonsten in einem lokalen\n",
        "# Environment. In Google Colab müsst Ihr links auf das Schlüssel-Symbol klicken\n",
        "# und den Key dort copy pasten sowie dem Key einen Namen geben. Diesen Namen\n",
        "# nutzt Ihr dann hier in der Klammer:\n",
        "\n",
        "from google.colab import userdata\n",
        "OPENAI_API_KEY = userdata.get('apikey_rh')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_openai"
      ],
      "metadata": {
        "id": "9u8WePp87ZQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "IVLOVSkZ2gf1"
      },
      "outputs": [],
      "source": [
        "# Nun nutzen wir ein LLM von OpenAI, indem wir die API direkt ansprechen, in\n",
        "# diesem Fall das gpt-4o-mini:\n",
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "AeoTvWd52gf2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c15f2bc-2880-4dfb-9b7a-216904db37b8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Der Sinn des Lebens ist eine subjektive Frage, die für jeden individuell ist. Viele Menschen finden Erfüllung durch Beziehungen, persönliche Entwicklung, das Streben nach Glück oder das Hinterlassen eines positiven Einflusses. Letztendlich liegt es an jedem Einzelnen, seinen eigenen Sinn zu entdecken.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 58, 'prompt_tokens': 19, 'total_tokens': 77, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': None, 'id': 'chatcmpl-Brk4bVObghfRuJuwS0OwR6ogmlRAS', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--c97377ff-ab9f-4d04-95d4-2e3098c083b1-0', usage_metadata={'input_tokens': 19, 'output_tokens': 58, 'total_tokens': 77, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "# Nun kann mit model.invoke() ein Prompt an die API geschickt werden, und das\n",
        "# in model eingestellte Modell liefert die Antwort:\n",
        "model.invoke(\"Was ist der Sinn des Lebens? Bitte fasse Dich kurz!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Wir können nun den Parser in Langchain verwenden, um nur die Antwort des\n",
        "# AIMessage-Objekts zu bekommen. Gleichzeitig nutzen wir die Verkettungs-\n",
        "# funktion von Langchain über das Pipe-Symbol |, d.h. wir können eine chain\n",
        "# von Arbeitsschritten bauen, in der der Output links von | der Input von rechts\n",
        "# von | wird.\n",
        "# In diesem Fall wollen wir, dass der Output des Modells in den Parser geht und\n",
        "# dieser nur die Antwort zurückgibt.\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "chain = model | parser\n",
        "chain.invoke(\"Was ist der Sinn des Lebens? Bitte fasse Dich kurz!\")"
      ],
      "metadata": {
        "id": "Vrgeja4wuYYs",
        "outputId": "4a4db7f8-cbf1-4c22-a7de-cb6e6c36cbe0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Der Sinn des Lebens ist eine philosophische Frage, die je nach persönlicher Überzeugung und Erfahrung unterschiedlich beantwortet wird. Viele Menschen finden Sinn in Beziehungen, persönlichem Wachstum, Liebe, Freude, einer sinnvollen Tätigkeit oder dem Streben nach Wissen. Letztlich liegt es an jedem Einzelnen, seinen eigenen Sinn zu definieren.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "bzyb8Q9j2gf3",
        "outputId": "7f65e595-e8f7-4379-dda0-f953fefbe296"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Human: \\nBeantworte die Frage basierend auf dem Kontext.\\nWenn Du die Frage nicht beantworten kannst, antworte \"Ich weiß es nicht\".\\n\\nContext: Ralfs Bruder heißt Axl.\\n\\nQuestion: Wer ist Ralfs Bruder?\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "# Nun können wir die Frage an das LLM auch besser strukturieren, indem wir\n",
        "# Frage und Kontext voneinander trennen. Hierfür bietet bspw. Langchain eine\n",
        "# Funktionalität namens ChatPromptTemplate an, mit der ein solches Template\n",
        "# gebaut werden kann.\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "template = \"\"\"\n",
        "Beantworte die Frage basierend auf dem Kontext.\n",
        "Wenn Du die Frage nicht beantworten kannst, antworte \"Ich weiß es nicht\".\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "prompt.format(context=\"Ralfs Bruder heißt Axl.\", question=\"Wer ist Ralfs Bruder?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "id": "yLvqUhAb2gf4",
        "outputId": "49454abd-84e6-4eb3-94c8-25976e23f475"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ralfs Bruder ist Axl.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "# Nun können wir wieder mit der Verkettungsfunktion von Langchain den Prompt\n",
        "# als Input in das Modell geben, und dessen Output wiederum an den Parser.\n",
        "chain = prompt | model | parser\n",
        "chain.invoke({\n",
        "    \"context\": \"Ralfs Bruder heißt Axl\",\n",
        "    \"question\": \"Wer ist Ralfs Bruder?\"\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "GzNNtXgQ2gf5"
      },
      "outputs": [],
      "source": [
        "# Wir können die Kette beliebig erweitern, z.B. um ein Übersetzungs-Template.\n",
        "translation_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Translate {answer} to {language}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "id": "lGBkJ1xe2gf5",
        "outputId": "0c90f6e1-d250-4759-8c16-98c29f678643"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ralf has a total of three siblings.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "# Jetzt können wir sogar eine Chain innerhalb einer Chain verwenden:\n",
        "# Wir verwenden die bisherige Chain (\"chain\"), um ihr \"context\" und \"question\"\n",
        "# (auf Deutsch) zu geben. Diese wird zusammen mit der Zielsprache an den\n",
        "# Übersetzungs-Promt übergeben, die dies wiederum an das Modell übergibt.\n",
        "from operator import itemgetter\n",
        "\n",
        "translation_chain = (\n",
        "    {\"answer\": chain, \"language\": itemgetter(\"language\")} | translation_prompt | model | parser\n",
        ")\n",
        "\n",
        "translation_chain.invoke(\n",
        "    {\n",
        "        \"context\": \"Ralfs Bruder heißt Axl. Er hat zwei weitere Geschwister.\",\n",
        "        \"question\": \"Wie viele Geschwister hat Ralf?\",\n",
        "        \"language\": \"English\",\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZC-Twgf2gf6",
        "outputId": "054e3684-5a92-413f-f686-a6e36b9e4544"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I think it's possible that physics has exploits and we should be trying to find them. arranging some\n"
          ]
        }
      ],
      "source": [
        "# Nun wollen wir mehr Kontext erlauben als nur zwei Sätze. Dazu verwenden wir\n",
        "# in diesem Beispiel ein Transkript von einem youtube-Video, wo Andrej Karpathy\n",
        "# über AI spricht:\n",
        "# https://www.youtube.com/watch?v=cdiD-9MMpb0\n",
        "# Das Transkript davon ist bereits erstellt und liegt in GitHub.\n",
        "import requests\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/RalfH1388/genai-lecture/main/data_interview.txt\"\n",
        "response = requests.get(url)\n",
        "interview = response.text\n",
        "\n",
        "print(interview[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "VvNjnFpA2gf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "a691e022-018d-4a4d-8c53-c4abecffef6c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Yes, reading papers can be a good idea, as they often provide insights into current research, methodologies, and developments in a field. They can help deepen your understanding of specific topics and keep you informed about advancements. However, it is also important to recognize that not all papers are equally valuable, and some may require careful evaluation to ascertain their relevance and quality.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "# Nun machen wir einen Schritt, der nicht empfehlenswert ist, aber trotzdem\n",
        "# einmal getan werden muss, um zu zeigen, wie es NICHT geht.\n",
        "# Eine naive Idee könnte jetzt sein, einfach das gesamte Textdokument mit dem\n",
        "# Interview als Kontext an das LLM zu geben:\n",
        "chain.invoke({\n",
        "  \"context\": interview,\n",
        "  \"question\": \"Is reading papers a good idea?\"\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Wie wir an der Antwort sehen, hat dies ganz gut funktioniert. Dies ist aber\n",
        "# nicht immer so, und hat außerdem zwei wichtige Limitationen (die zugleich\n",
        "# konstituierende Gründe sind für die Verwendung von RAG anstatt plain-LLM):\n",
        "# - viele benötigte Kontexte (bspw. firmeninterne 100-seitige Dokumente) sind\n",
        "#   für die context windows der jeweiligen LLMs zu groß, sprich: das LLM ist\n",
        "#   gar nicht in der Lage, so viele Tokens auf einmal in einer Anfrage zu\n",
        "#   bearbeiten (abgesehen davon, dass es kostenintensiv ist)\n",
        "# - selbst wenn irgendwann einmal LLMs ausreichend hohe context windows hätten,\n",
        "#   macht es immer noch wenig Sinn, zu viel Kontext mitzugeben. Wenn man einfach\n",
        "#   alles mitgibt, kann das Modell den Fokus verlieren oder irrelevante Teile\n",
        "#   berücksichtigen.\n",
        "# Hier kommt nun die Idee von RAG in's Spiel: wir wollen nur relevanten Kontext\n",
        "# als Input für das Modell verwenden. Die Frage ist nur: wie kommen wir dahin,\n",
        "# a priori relevanten von irrelevantem Kontext zu unterscheiden?\n",
        "# Antwort: wir unterteilen unseren Kontext in kleinere Teile (sog. Chunks),\n",
        "# und wollen nun die gestellt Frage mit all diesen Chunks vergleichen, und nur\n",
        "# die Chunks an das Modell geben, die am Relevantesten sind. Nun bleibt aber\n",
        "# immer noch die Frage offen: wie entscheiden wir, welche Chunks am\n",
        "# Relevantesten sind? Die Antwort kommt gleich (Stichwort: Embeddings), aber\n",
        "# zunächst mal kümmern wir uns um das Chunking des Interviews."
      ],
      "metadata": {
        "id": "DdIw28ohHahv"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community"
      ],
      "metadata": {
        "id": "yyv0EoJyDLSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "csTqTilc2gf7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb7bd489-a731-4f41-fafb-8306f844eea2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I think it's possible that physics has exploits and we should be trying to find them. arranging some kind of a crazy quantum mechanical system that somehow gives you buffer overflow, somehow gives you a rounding error in the floating point. Synthetic intelligences are kind of like the next stage of development. And I don't know where it leads to. Like at some point, I suspect the universe is some kind of a puzzle. These synthetic AIs will uncover that puzzle and solve it. The following is a conv\n"
          ]
        }
      ],
      "source": [
        "# Hier für gibt es wieder vordefinerte Frameworks, die den Text in etwa gleiche\n",
        "# Teile aufteilen. Dazu müssen wir zunächst das Interview in die benötigten\n",
        "# Strukuten laden (der unten stehende Code ist etwas hemdsärmelig, es ist nicht\n",
        "# direkt wichtig ihn zu verstehen)\n",
        "import requests\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "import tempfile\n",
        "\n",
        "# Datei herunterladen\n",
        "response = requests.get(url)\n",
        "response.raise_for_status()\n",
        "\n",
        "# Temporäre Datei schreiben\n",
        "with tempfile.NamedTemporaryFile(delete=False, suffix=\".txt\", mode=\"w\") as tmp:\n",
        "    tmp.write(response.text)\n",
        "    temp_path = tmp.name\n",
        "\n",
        "# Mit TextLoader laden\n",
        "loader = TextLoader(temp_path)\n",
        "text_documents = loader.load()\n",
        "\n",
        "print(text_documents[0].page_content[:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJHrcHS-2gf8",
        "outputId": "14acbdfd-9c49-487c-d7af-ee67247c4bbd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': '/tmp/tmpddgnw7vd.txt'}, page_content=\"I think it's possible that physics has exploits and we should be trying to find them. arranging some\"),\n",
              " Document(metadata={'source': '/tmp/tmpddgnw7vd.txt'}, page_content='arranging some kind of a crazy quantum mechanical system that somehow gives you buffer overflow,'),\n",
              " Document(metadata={'source': '/tmp/tmpddgnw7vd.txt'}, page_content='buffer overflow, somehow gives you a rounding error in the floating point. Synthetic intelligences'),\n",
              " Document(metadata={'source': '/tmp/tmpddgnw7vd.txt'}, page_content=\"intelligences are kind of like the next stage of development. And I don't know where it leads to.\"),\n",
              " Document(metadata={'source': '/tmp/tmpddgnw7vd.txt'}, page_content='where it leads to. Like at some point, I suspect the universe is some kind of a puzzle. These')]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "# Natürlich bietet Langchain nun auch ein Framework, wie man die Chunks ohne\n",
        "# explizites Coding erstellen kann. In diesem Fall erstellt untenstehende\n",
        "# Funktionalität Chunks, bei denen jeder Chunk in etwa 100 Zeichen hat (die\n",
        "# Prozedur schneidet nicht hart beim 101. Zeichen ab, sondern versucht, Worte\n",
        "# nicht auseinanderzureißen) und einen Overlapt von 20 Zeichen (d.h. ca. die\n",
        "# letzten 20 Zeichen von Chunk i sind auch ca. die ersten 20 Zeichen von Chunk\n",
        "# i+1). Warum? Beim Splitten kann es passieren, dass ein wichtiger Gedanke,\n",
        "# Satz oder Begriff genau am Ende eines Chunks aufhört – und dann im nächsten\n",
        "# Chunk außer Kontext steht. Durch Overlap wird ein Teil des vorherigen Kontexts\n",
        "# in den nächsten Chunk übernommen, damit dieser \"weiß\", worum es vorher ging.\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
        "text_splitter.split_documents(text_documents)[:5]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Auch bei der Größe der Chungs sollte man gewisse Grenzen einhalten. Zu kleine\n",
        "# Chunks (< 100 Tokens) enthalten oft zu wenig Kontext, und führen\n",
        "# zu sinnlosen Embeddings (z. B. nur „Abschnitt 2.1: Einleitung“).mZu große\n",
        "# Chunks (> 1000 Tokens) erschweren das Matching bei Embedding-Suche (Embeddings\n",
        "# sind zu unspezifisch), können bei LLMs zu Token-Limit-Problemen führen, und\n",
        "# senken die Präzision bei Kontext-Retrieval („too much noise“)."
      ],
      "metadata": {
        "id": "UkDVS8cbKdCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "IdJYIbUp2gf8"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
        "documents = text_splitter.split_documents(text_documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZHYMquf2gf8",
        "outputId": "8825fb83-16c1-462d-9c69-5c91ee9ec087"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding length: 1536\n",
            "[-0.024147924035787582, 0.025537647306919098, -0.020976504310965538, -0.015940243378281593, -0.034351106733083725, 0.0030080974102020264, -0.007103029638528824, 0.0172111876308918, -0.042903248220682144, -0.038057032972574234]\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(\n",
        "    model=\"text-embedding-3-small\",\n",
        "    openai_api_key=OPENAI_API_KEY\n",
        ")\n",
        "\n",
        "embedded_query = embeddings.embed_query(\"Wer ist Ralfs Bruder?\")\n",
        "\n",
        "print(f\"Embedding length: {len(embedded_query)}\")\n",
        "print(embedded_query[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "YdWRgmLD2gf9"
      },
      "outputs": [],
      "source": [
        "sentence1 = embeddings.embed_query(\"Ralfs Bruder ist Axl\")\n",
        "sentence2 = embeddings.embed_query(\"Michaelas Bruder ist Paul\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McA_WYRW2gf9",
        "outputId": "c57e89ec-c415-41ea-b140-d9ce554627ec"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(np.float64(0.7451138269282955), np.float64(0.4752776876358714))"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "query_sentence1_similarity = cosine_similarity([embedded_query], [sentence1])[0][0]\n",
        "query_sentence2_similarity = cosine_similarity([embedded_query], [sentence2])[0][0]\n",
        "\n",
        "query_sentence1_similarity, query_sentence2_similarity"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install docarray"
      ],
      "metadata": {
        "id": "3ybMgoOz8jcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "vzooiuLF2gf-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "cf9f2b9d-44df-4699-ac42-06cea06dc884"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "PermissionDeniedError",
          "evalue": "Error code: 403 - {'error': {'message': 'Project `proj_IYGimIkXl9Jrvw6o15RHOQYt` does not have access to model `text-embedding-3-small`', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPermissionDeniedError\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-57-697329805.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_community\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorstores\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDocArrayInMemorySearch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m vectorstore1 = DocArrayInMemorySearch.from_texts(\n\u001b[0m\u001b[1;32m      4\u001b[0m     [\n\u001b[1;32m      5\u001b[0m         \u001b[0;34m\"Ralfs Bruder heißt Axl\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_community/vectorstores/docarray/in_memory.py\u001b[0m in \u001b[0;36mfrom_texts\u001b[0;34m(cls, texts, embedding, metadatas, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \"\"\"\n\u001b[1;32m     67\u001b[0m         \u001b[0mstore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadatas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadatas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_community/vectorstores/docarray/base.py\u001b[0m in \u001b[0;36madd_texts\u001b[0;34m(self, texts, metadatas, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \"\"\"\n\u001b[1;32m     81\u001b[0m         \u001b[0mids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadatas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmetadatas\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_openai/embeddings/base.py\u001b[0m in \u001b[0;36membed_documents\u001b[0;34m(self, texts, chunk_size, **kwargs)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;31m#       than the maximum context and use length-safe embedding function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0mengine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeployment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         return self._get_len_safe_embeddings(\n\u001b[0m\u001b[1;32m    591\u001b[0m             \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_openai/embeddings/base.py\u001b[0m in \u001b[0;36m_get_len_safe_embeddings\u001b[0;34m(self, texts, engine, chunk_size, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0mbatched_embeddings\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m             response = self.client.create(\n\u001b[0m\u001b[1;32m    479\u001b[0m                 \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0m_chunk_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mclient_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/embeddings.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    130\u001b[0m             \u001b[0;34m\"/embeddings\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaybe_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_create_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbeddingCreateParams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1247\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m         )\n\u001b[0;32m-> 1249\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPermissionDeniedError\u001b[0m: Error code: 403 - {'error': {'message': 'Project `proj_IYGimIkXl9Jrvw6o15RHOQYt` does not have access to model `text-embedding-3-small`', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}"
          ]
        }
      ],
      "source": [
        "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
        "\n",
        "vectorstore1 = DocArrayInMemorySearch.from_texts(\n",
        "    [\n",
        "        \"Ralfs Bruder heißt Axl\",\n",
        "        \"Michaela und Paul sind Geschwister\",\n",
        "        \"Dennis mag weiße Autos\",\n",
        "        \"Anna Mutter ist Lehrerin\",\n",
        "        \"Hektor fährt einen schwarzen Audi\",\n",
        "        \"Michaela hat zwei Geschwister\",\n",
        "    ],\n",
        "    embedding=embeddings,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QD7XZInT2gf-",
        "outputId": "cb5274de-fc56-42a0-f79b-16226c59d92a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(Document(metadata={}, page_content='Ralfs Bruder heißt Axl'),\n",
              "  np.float64(0.9128045266088952)),\n",
              " (Document(metadata={}, page_content='Michaela und Paul sind Geschwister'),\n",
              "  np.float64(0.8209296823604758)),\n",
              " (Document(metadata={}, page_content='Michaela hat zwei Geschwister'),\n",
              "  np.float64(0.8029525892979162))]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "vectorstore1.similarity_search_with_score(query=\"Wer ist Ralfs Bruder?\", k=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jt_ui5w2gf_",
        "outputId": "16e4ebce-d2d0-4ff6-acfe-08958df0642a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={}, page_content='Ralfs Bruder heißt Axl'),\n",
              " Document(metadata={}, page_content='Michaela und Paul sind Geschwister'),\n",
              " Document(metadata={}, page_content='Michaela hat zwei Geschwister'),\n",
              " Document(metadata={}, page_content='Hektor fährt einen schwarzen Audi')]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "retriever1 = vectorstore1.as_retriever()\n",
        "retriever1.invoke(\"Wer ist Ralfs Bruder?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lx8Liegh2gf_",
        "outputId": "c18fca27-a455-490d-e2e6-d478710bbb7f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'context': [Document(metadata={}, page_content='Hektor fährt einen schwarzen Audi'),\n",
              "  Document(metadata={}, page_content='Dennis mag weiße Autos'),\n",
              "  Document(metadata={}, page_content='Ralfs Bruder heißt Axl'),\n",
              "  Document(metadata={}, page_content='Michaela hat zwei Geschwister')],\n",
              " 'question': 'Welche Farbe hat Hektors Auto?'}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
        "\n",
        "setup = RunnableParallel(context=retriever1, question=RunnablePassthrough())\n",
        "setup.invoke(\"Welche Farbe hat Hektors Auto?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "id": "QFS2WV8f2ggA",
        "outputId": "77241107-a7a6-4e95-a0bd-92681804b77d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hektors Auto ist schwarz.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "chain = setup | prompt | model | parser\n",
        "chain.invoke(\"Welche Farbe hat Hektors Auto?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "id": "QmntjKf42ggA",
        "outputId": "df839970-b52a-4281-f526-a0f832d120ad"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hektor fährt einen schwarzen Audi.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "chain.invoke(\"Welches Auto fährt Hektor?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctfsZEPU2ggB"
      },
      "outputs": [],
      "source": [
        "vectorstore2 = DocArrayInMemorySearch.from_documents(documents, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "id": "jpcxrRQi2ggB",
        "outputId": "8a1a1104-7ef4-4972-d293-551cb3c779ad"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Synthetic intelligence refers to advanced artificial intelligence systems that are seen as the next stage of development in AI. These systems are thought to possess capabilities that allow them to uncover and solve complex problems or puzzles within the universe. Unlike traditional forms of AI, synthetic intelligences are anticipated to engage in tasks that involve understanding emotions, creativity, and generating art and ideas autonomously.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "chain = (\n",
        "    {\"context\": vectorstore2.as_retriever(), \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | parser\n",
        ")\n",
        "chain.invoke(\"What is synthetic intelligence?\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}