{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RalfH1388/genai-lecture/blob/main/genai_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieval Augmented Generation (RAG)\n",
        "# ---\n",
        "# In diesem Beispiel erstellen wir ein RAG-System.\n",
        "# Warum ist RAG sinnvoll? Weil ein LLM nicht per se inhaltlich korrekten Inhalt\n",
        "# produziert, sondern wie ein \"stochastischer Papagei\" entlang seiner Trainings-\n",
        "# daten Tokens basierend auf vorherigen Tokens produziert.\n",
        "# Wenn wir uns sicher sein wollen, dass die Inhalte korrekt sind, brauchen wir\n",
        "# also andere Methoden. Und manchmal ist es ohnehin so, dass wir sehr spezielle\n",
        "# Informationen verarbeiten müssen (z.B. geheime firmeninterne Dokumente), von\n",
        "# denen das LLM sowieso nicht direkt etwas Bescheid weiß.\n",
        "# Die Grundidee eines RAG-Systems ist, dass ein Sprachmodell bei der\n",
        "# Beantwortung von Fragen externe Wissensquellen durchsucht und relevante\n",
        "# Informationen in seine Antwort integriert."
      ],
      "metadata": {
        "id": "UiQ1Su33M_Lf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4c9Oe7l32gf0"
      },
      "outputs": [],
      "source": [
        "# Wir wollen hier mit den LLMs von OpenAI arbeiten. Da wir ein Backend-System\n",
        "# entwickeln, hilft uns die UI-Version von ChatGPT nicht weiter, sondern wir\n",
        "# brauchen Zugriff zur Developer-API mittels API Key. Jeder von Euch hat von mir\n",
        "# entweder einen API Key von meinem persönlichen Account bekommen, oder nutzt\n",
        "# einen aus seinem eigenen Account. Dieser Key muss - falls Ihr in Google\n",
        "# Colab bleibt - als Secret hier hinterlegt werden, ansonsten in einem lokalen\n",
        "# Environment. In Google Colab müsst Ihr links auf das Schlüssel-Symbol klicken\n",
        "# und den Key dort copy pasten sowie dem Key einen Namen geben. Diesen Namen\n",
        "# nutzt Ihr dann hier in der Klammer:\n",
        "\n",
        "from google.colab import userdata\n",
        "OPENAI_API_KEY = userdata.get('apikey_rh')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_openai"
      ],
      "metadata": {
        "id": "9u8WePp87ZQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "IVLOVSkZ2gf1"
      },
      "outputs": [],
      "source": [
        "# Nun nutzen wir ein LLM von OpenAI, indem wir die API direkt ansprechen, in\n",
        "# diesem Fall das gpt-4o-mini:\n",
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AeoTvWd52gf2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efe8c03c-aee0-4a47-ff11-0ac8091f39de"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Der Sinn des Lebens ist eine individuelle Frage, die für jeden Menschen anders beantwortet werden kann. Viele finden Sinn in Beziehungen, persönlichem Wachstum, der Verfolgung von Leidenschaften, dem Streben nach Glück oder dem Beitrag zur Gesellschaft. Letztendlich hängt es von den eigenen Werten und Zielen ab.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 63, 'prompt_tokens': 19, 'total_tokens': 82, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-BrnSUI5WFUKXWMJyAH9y0LrN1WvsB', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--2f47533c-8e84-4c0f-adff-cadde4d05b41-0', usage_metadata={'input_tokens': 19, 'output_tokens': 63, 'total_tokens': 82, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Nun kann mit model.invoke() ein Prompt an die API geschickt werden, und das\n",
        "# in model eingestellte Modell liefert die Antwort:\n",
        "model.invoke(\"Was ist der Sinn des Lebens? Bitte fasse Dich kurz!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Wir können nun den Parser in Langchain verwenden, um nur die Antwort des\n",
        "# AIMessage-Objekts zu bekommen. Gleichzeitig nutzen wir die Verkettungs-\n",
        "# funktion von Langchain über das Pipe-Symbol |, d.h. wir können eine chain\n",
        "# von Arbeitsschritten bauen, in der der Output links von | der Input von rechts\n",
        "# von | wird.\n",
        "# In diesem Fall wollen wir, dass der Output des Modells in den Parser geht und\n",
        "# dieser nur die Antwort zurückgibt.\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "chain = model | parser\n",
        "chain.invoke(\"Was ist der Sinn des Lebens? Bitte fasse Dich kurz!\")"
      ],
      "metadata": {
        "id": "Vrgeja4wuYYs",
        "outputId": "a2d9c40e-1a2a-4acc-e641-124de86fc6e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Der Sinn des Lebens ist eine individuelle Frage und kann für jeden Menschen unterschiedlich sein. Viele Menschen finden Sinn in Beziehungen, persönlichem Wachstum, Glück, Liebe, Sinnstiftung durch Arbeit oder das Streben nach Wissen und Erfahrungen. Letztlich geht es darum, einen eigenen Lebensweg zu finden und Bedeutung in den eigenen Erlebnissen und Zielen zu erkennen.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "bzyb8Q9j2gf3",
        "outputId": "d1b0888d-a331-4e52-933c-04c83cb1a3fd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Human: \\nBeantworte die Frage basierend auf dem Kontext.\\nWenn Du die Frage nicht beantworten kannst, antworte \"Ich weiß es nicht\".\\n\\nContext: Ralfs Bruder heißt Axl.\\n\\nQuestion: Wer ist Ralfs Bruder?\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Nun können wir die Frage an das LLM auch besser strukturieren, indem wir\n",
        "# Frage und Kontext voneinander trennen. Hierfür bietet bspw. Langchain eine\n",
        "# Funktionalität namens ChatPromptTemplate an, mit der ein solches Template\n",
        "# gebaut werden kann.\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "template = \"\"\"\n",
        "Beantworte die Frage basierend auf dem Kontext.\n",
        "Wenn Du die Frage nicht beantworten kannst, antworte \"Ich weiß es nicht\".\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "prompt.format(context=\"Ralfs Bruder heißt Axl.\", question=\"Wer ist Ralfs Bruder?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "id": "yLvqUhAb2gf4",
        "outputId": "13a795e7-8f61-4f40-c2df-0f5feea5dd50"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ralfs Bruder ist Axl.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# Nun können wir wieder mit der Verkettungsfunktion von Langchain den Prompt\n",
        "# als Input in das Modell geben, und dessen Output wiederum an den Parser.\n",
        "chain = prompt | model | parser\n",
        "chain.invoke({\n",
        "    \"context\": \"Ralfs Bruder heißt Axl\",\n",
        "    \"question\": \"Wer ist Ralfs Bruder?\"\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "GzNNtXgQ2gf5"
      },
      "outputs": [],
      "source": [
        "# Wir können die Kette beliebig erweitern, z.B. um ein Übersetzungs-Template.\n",
        "translation_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Translate {answer} to {language}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "id": "lGBkJ1xe2gf5",
        "outputId": "f3817c55-69f4-4a49-b1d8-0ac7dca0cfbf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ralf has a total of three siblings.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Jetzt können wir sogar eine Chain innerhalb einer Chain verwenden:\n",
        "# Wir verwenden die bisherige Chain (\"chain\"), um ihr \"context\" und \"question\"\n",
        "# (auf Deutsch) zu geben. Diese wird zusammen mit der Zielsprache an den\n",
        "# Übersetzungs-Promt übergeben, die dies wiederum an das Modell übergibt.\n",
        "from operator import itemgetter\n",
        "\n",
        "translation_chain = (\n",
        "    {\"answer\": chain, \"language\": itemgetter(\"language\")} | translation_prompt | model | parser\n",
        ")\n",
        "\n",
        "translation_chain.invoke(\n",
        "    {\n",
        "        \"context\": \"Ralfs Bruder heißt Axl. Er hat zwei weitere Geschwister.\",\n",
        "        \"question\": \"Wie viele Geschwister hat Ralf?\",\n",
        "        \"language\": \"English\",\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZC-Twgf2gf6",
        "outputId": "6d7fc982-2742-45a5-cf50-8fd107efee57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I think it's possible that physics has exploits and we should be trying to find them. arranging some\n"
          ]
        }
      ],
      "source": [
        "# Nun wollen wir mehr Kontext erlauben als nur zwei Sätze. Dazu verwenden wir\n",
        "# in diesem Beispiel ein Transkript von einem youtube-Video, wo Andrej Karpathy\n",
        "# über AI spricht:\n",
        "# https://www.youtube.com/watch?v=cdiD-9MMpb0\n",
        "# Das Transkript davon ist bereits erstellt und liegt in GitHub.\n",
        "import requests\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/RalfH1388/genai-lecture/main/data_interview.txt\"\n",
        "response = requests.get(url)\n",
        "interview = response.text\n",
        "\n",
        "print(interview[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "VvNjnFpA2gf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "baedbbe4-fa0c-46dc-e2c3-cbb688aa701a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Yes, reading papers is a good idea because they provide insights into the latest research, methodologies, and developments in a specific field. They can serve as a source of knowledge that helps you understand complex concepts, discover new ideas, and stay informed about advancements. Papers often contain detailed information that can deepen your understanding, especially when paired with hands-on experience or experimentation. However, it's important to critically evaluate the content of the papers and recognize that not all research may be directly applicable or immediately useful.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# Nun machen wir einen Schritt, der nicht empfehlenswert ist, aber trotzdem\n",
        "# einmal getan werden muss, um zu zeigen, wie es NICHT geht.\n",
        "# Eine naive Idee könnte jetzt sein, einfach das gesamte Textdokument mit dem\n",
        "# Interview als Kontext an das LLM zu geben:\n",
        "chain.invoke({\n",
        "  \"context\": interview,\n",
        "  \"question\": \"Is reading papers a good idea?\"\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Wie wir an der Antwort sehen, hat dies ganz gut funktioniert. Dies ist aber\n",
        "# nicht immer so, und hat außerdem zwei wichtige Limitationen (die zugleich\n",
        "# konstituierende Gründe sind für die Verwendung von RAG anstatt plain-LLM):\n",
        "# - viele benötigte Kontexte (bspw. firmeninterne 100-seitige Dokumente) sind\n",
        "#   für die context windows der jeweiligen LLMs zu groß, sprich: das LLM ist\n",
        "#   gar nicht in der Lage, so viele Tokens auf einmal in einer Anfrage zu\n",
        "#   bearbeiten (abgesehen davon, dass es kostenintensiv ist)\n",
        "# - selbst wenn irgendwann einmal LLMs ausreichend hohe context windows hätten,\n",
        "#   macht es immer noch wenig Sinn, zu viel Kontext mitzugeben. Wenn man einfach\n",
        "#   alles mitgibt, kann das Modell den Fokus verlieren oder irrelevante Teile\n",
        "#   berücksichtigen.\n",
        "# Hier kommt nun die Idee von RAG in's Spiel: wir wollen nur relevanten Kontext\n",
        "# als Input für das Modell verwenden. Die Frage ist nur: wie kommen wir dahin,\n",
        "# a priori relevanten von irrelevantem Kontext zu unterscheiden?\n",
        "# Antwort: wir unterteilen unseren Kontext in kleinere Teile (sog. Chunks),\n",
        "# und wollen nun die gestellt Frage mit all diesen Chunks vergleichen, und nur\n",
        "# die Chunks an das Modell geben, die am Relevantesten sind. Nun bleibt aber\n",
        "# immer noch die Frage offen: wie entscheiden wir, welche Chunks am\n",
        "# Relevantesten sind? Die Antwort kommt gleich (Stichwort: Embeddings), aber\n",
        "# zunächst mal kümmern wir uns um das Chunking des Interviews."
      ],
      "metadata": {
        "id": "DdIw28ohHahv"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community"
      ],
      "metadata": {
        "id": "yyv0EoJyDLSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "csTqTilc2gf7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7d7c049-b2f7-4f50-e75e-58bdef7f221b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I think it's possible that physics has exploits and we should be trying to find them. arranging some kind of a crazy quantum mechanical system that somehow gives you buffer overflow, somehow gives you a rounding error in the floating point. Synthetic intelligences are kind of like the next stage of development. And I don't know where it leads to. Like at some point, I suspect the universe is some kind of a puzzle. These synthetic AIs will uncover that puzzle and solve it. The following is a conv\n"
          ]
        }
      ],
      "source": [
        "# Hier für gibt es wieder vordefinerte Frameworks, die den Text in etwa gleiche\n",
        "# Teile aufteilen. Dazu müssen wir zunächst das Interview in die benötigten\n",
        "# Strukuten laden (der unten stehende Code ist etwas hemdsärmelig, es ist nicht\n",
        "# direkt wichtig ihn zu verstehen)\n",
        "import requests\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "import tempfile\n",
        "\n",
        "# Datei herunterladen\n",
        "response = requests.get(url)\n",
        "response.raise_for_status()\n",
        "\n",
        "# Temporäre Datei schreiben\n",
        "with tempfile.NamedTemporaryFile(delete=False, suffix=\".txt\", mode=\"w\") as tmp:\n",
        "    tmp.write(response.text)\n",
        "    temp_path = tmp.name\n",
        "\n",
        "# Mit TextLoader laden\n",
        "loader = TextLoader(temp_path)\n",
        "text_documents = loader.load()\n",
        "\n",
        "print(text_documents[0].page_content[:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJHrcHS-2gf8",
        "outputId": "95100520-a378-4ba7-8670-b3819e0942aa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': '/tmp/tmp_tngm9if.txt'}, page_content=\"I think it's possible that physics has exploits and we should be trying to find them. arranging some\"),\n",
              " Document(metadata={'source': '/tmp/tmp_tngm9if.txt'}, page_content='arranging some kind of a crazy quantum mechanical system that somehow gives you buffer overflow,'),\n",
              " Document(metadata={'source': '/tmp/tmp_tngm9if.txt'}, page_content='buffer overflow, somehow gives you a rounding error in the floating point. Synthetic intelligences'),\n",
              " Document(metadata={'source': '/tmp/tmp_tngm9if.txt'}, page_content=\"intelligences are kind of like the next stage of development. And I don't know where it leads to.\"),\n",
              " Document(metadata={'source': '/tmp/tmp_tngm9if.txt'}, page_content='where it leads to. Like at some point, I suspect the universe is some kind of a puzzle. These')]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "# Natürlich bietet Langchain nun auch ein Framework, wie man die Chunks ohne\n",
        "# explizites Coding erstellen kann. In diesem Fall erstellt untenstehende\n",
        "# Funktionalität Chunks, bei denen jeder Chunk in etwa 100 Zeichen hat (die\n",
        "# Prozedur schneidet nicht hart beim 101. Zeichen ab, sondern versucht, Worte\n",
        "# nicht auseinanderzureißen) und einen Overlapt von 20 Zeichen (d.h. ca. die\n",
        "# letzten 20 Zeichen von Chunk i sind auch ca. die ersten 20 Zeichen von Chunk\n",
        "# i+1). Warum? Beim Splitten kann es passieren, dass ein wichtiger Gedanke,\n",
        "# Satz oder Begriff genau am Ende eines Chunks aufhört – und dann im nächsten\n",
        "# Chunk außer Kontext steht. Durch Overlap wird ein Teil des vorherigen Kontexts\n",
        "# in den nächsten Chunk übernommen, damit dieser \"weiß\", worum es vorher ging.\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
        "text_splitter.split_documents(text_documents)[:5]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Auch bei der Größe der Chungs sollte man gewisse Grenzen einhalten. Zu kleine\n",
        "# Chunks (< 100 Tokens) enthalten oft zu wenig Kontext, und führen\n",
        "# zu sinnlosen Embeddings (z. B. nur „Abschnitt 2.1: Einleitung“).mZu große\n",
        "# Chunks (> 1000 Tokens) erschweren das Matching bei Embedding-Suche (Embeddings\n",
        "# sind zu unspezifisch), können bei LLMs zu Token-Limit-Problemen führen, und\n",
        "# senken die Präzision bei Kontext-Retrieval („too much noise“)."
      ],
      "metadata": {
        "id": "UkDVS8cbKdCj"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZHYMquf2gf8",
        "outputId": "a5230178-fbd1-4321-b7ff-a5ded98a11cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding length: 1536\n",
            "[-0.024147924035787582, 0.025537647306919098, -0.020976504310965538, -0.015940243378281593, -0.034351106733083725, 0.0030080974102020264, -0.007103029638528824, 0.0172111876308918, -0.042903248220682144, -0.038057032972574234]\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(\n",
        "    model=\"text-embedding-3-small\",\n",
        "    openai_api_key=OPENAI_API_KEY\n",
        ")\n",
        "\n",
        "embedded_query = embeddings.embed_query(\"Wer ist Ralfs Bruder?\")\n",
        "\n",
        "print(f\"Embedding length: {len(embedded_query)}\")\n",
        "print(embedded_query[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "YdWRgmLD2gf9"
      },
      "outputs": [],
      "source": [
        "sentence1 = embeddings.embed_query(\"Ralfs Bruder ist Axl\")\n",
        "sentence2 = embeddings.embed_query(\"Michaelas Bruder ist Paul\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McA_WYRW2gf9",
        "outputId": "2fb15ca3-346b-4766-8cc7-28723fa24372"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(np.float64(0.7451138269282955), np.float64(0.4752776876358714))"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "query_sentence1_similarity = cosine_similarity([embedded_query], [sentence1])[0][0]\n",
        "query_sentence2_similarity = cosine_similarity([embedded_query], [sentence2])[0][0]\n",
        "\n",
        "query_sentence1_similarity, query_sentence2_similarity"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install docarray"
      ],
      "metadata": {
        "id": "3ybMgoOz8jcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "vzooiuLF2gf-"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
        "\n",
        "vectorstore1 = DocArrayInMemorySearch.from_texts(\n",
        "    [\n",
        "        \"Ralfs Bruder heißt Axl\",\n",
        "        \"Michaela und Paul sind Geschwister\",\n",
        "        \"Dennis mag weiße Autos\",\n",
        "        \"Anna Mutter ist Lehrerin\",\n",
        "        \"Hektor fährt einen schwarzen Audi\",\n",
        "        \"Michaela hat zwei Geschwister\",\n",
        "    ],\n",
        "    embedding=embeddings,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QD7XZInT2gf-",
        "outputId": "a46228ca-c498-4990-dc0d-355864c12a57"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(Document(metadata={}, page_content='Ralfs Bruder heißt Axl'),\n",
              "  np.float64(0.7258295070391788)),\n",
              " (Document(metadata={}, page_content='Michaela und Paul sind Geschwister'),\n",
              "  np.float64(0.45822893726156455)),\n",
              " (Document(metadata={}, page_content='Michaela hat zwei Geschwister'),\n",
              "  np.float64(0.3977089176045281))]"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ],
      "source": [
        "vectorstore1.similarity_search_with_score(query=\"Wer ist Ralfs Bruder?\", k=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jt_ui5w2gf_",
        "outputId": "044a6946-6d42-4d08-dd0e-bb34715522c2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={}, page_content='Ralfs Bruder heißt Axl'),\n",
              " Document(metadata={}, page_content='Michaela und Paul sind Geschwister'),\n",
              " Document(metadata={}, page_content='Michaela hat zwei Geschwister'),\n",
              " Document(metadata={}, page_content='Hektor fährt einen schwarzen Audi')]"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ],
      "source": [
        "retriever1 = vectorstore1.as_retriever()\n",
        "retriever1.invoke(\"Wer ist Ralfs Bruder?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lx8Liegh2gf_",
        "outputId": "a2388631-6c21-4fdf-c455-fffc7ad77aef"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'context': [Document(metadata={}, page_content='Hektor fährt einen schwarzen Audi'),\n",
              "  Document(metadata={}, page_content='Dennis mag weiße Autos'),\n",
              "  Document(metadata={}, page_content='Ralfs Bruder heißt Axl'),\n",
              "  Document(metadata={}, page_content='Michaela hat zwei Geschwister')],\n",
              " 'question': 'Welche Farbe hat Hektors Auto?'}"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ],
      "source": [
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
        "\n",
        "setup = RunnableParallel(context=retriever1, question=RunnablePassthrough())\n",
        "setup.invoke(\"Welche Farbe hat Hektors Auto?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "id": "QFS2WV8f2ggA",
        "outputId": "96e9bc99-f717-40ad-9bbc-a7a2bd2457e5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hektors Auto ist schwarz.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 89
        }
      ],
      "source": [
        "chain = setup | prompt | model | parser\n",
        "chain.invoke(\"Welche Farbe hat Hektors Auto?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "id": "QmntjKf42ggA",
        "outputId": "f7aa40c3-8007-449e-b7cf-654197d1e1cd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hektor fährt einen schwarzen Audi.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 90
        }
      ],
      "source": [
        "chain.invoke(\"Welches Auto fährt Hektor?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "ctfsZEPU2ggB"
      },
      "outputs": [],
      "source": [
        "vectorstore2 = DocArrayInMemorySearch.from_documents(text_documents, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "jpcxrRQi2ggB",
        "outputId": "c39176f9-dece-4438-acc8-133a6cf431ab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Synthetic intelligence refers to artificial intelligence systems that simulate human-like cognitive functions, such as learning, reasoning, and problem-solving. It encompasses advanced algorithms and models designed to process information in ways that mimic human thought processes. These systems are often seen as the next stage in the development of AI, with the potential to uncover deeper insights and solve complex problems across various fields.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 94
        }
      ],
      "source": [
        "chain = (\n",
        "    {\"context\": vectorstore2.as_retriever(), \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | parser\n",
        ")\n",
        "chain.invoke(\"What is synthetic intelligence? Please answer in few sentences!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}